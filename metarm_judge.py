import argparse
import copy
import json
import os
import random
import re
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from tqdm import tqdm

CURRENT_DIR = Path(__file__).resolve().parent
sys.path.insert(0, str(CURRENT_DIR))
from request_api import request_internal_conv_api


DEFAULT_METARM_MODEL = "gemini-3-pro"
DEFAULT_TEMPERATURE = 0.4
DEFAULT_MAX_TOKENS = 16 * 1024
DEFAULT_NUM_THREADS = 64
DEFAULT_OUTER_RETRIES = 10


META_JUDGE_PROMPT_V1 = """# Role
You are a professional RLHF data quality evaluation expert. Your task is to assess whether the “evaluation rationale” generated by a Reward Model (GenRM) accurately captures the core reasoning of a human expert (Golden Judge).

# Input Data

Below are the conversation context and the responses from two models:

{context_and_responses}

Below is the evaluation provided by the human expert (Golden Judge):
<golden_judge>
{golden_explanation}
</golden_judge>

Below is the evaluation generated by the model under review (GenRM):
<genrm_output>
{genrm_explanation}
</genrm_output>

# Evaluation Steps (Chain of Thought)

Please take a deep breath and proceed step by step with the following analysis:

1. **Extract Golden Key Points**:
   Read the explanation in <golden_judge> and identify the **core decisive factors** (Key Discriminators) that led to the final judgment (e.g., A > B).

   * Was it a factual error (hallucination)?
   * Was it a failure in instruction following?
   * Was it an issue of tone, formatting, or safety?
   * *Note: Ignore generic politeness or boilerplate comments. Focus only on the specific logic that differentiates the quality of A and B.*

2. **Check GenRM Coverage**:
   Read the explanation in <genrm_output> and determine whether it **explicitly identifies** the above “core decisive factors.”

   * If Golden says “A is wrong due to a math error,” but GenRM says “A is wrong due to poor tone,” even if both ultimately choose B as better, this is **Incorrect** (because the reasoning does not align and may be a lucky guess).
   * If GenRM only provides vague statements (e.g., “A is more detailed than B”) without pointing out the specific issues emphasized by Golden, this is also **Incorrect**.

3. **Final Decision**:

   * If GenRM’s reasoning is consistent with Golden’s core logic (even if phrased differently), the verdict is **Correct**.
   * If GenRM misses key error points, fabricates reasons that do not exist, or conflicts with Golden’s logic, the verdict is **Incorrect**.

# Output Format

Please strictly follow the XML format below when outputting your analysis and final conclusion:

<golden_key_points>
Briefly summarize the key points that the Golden Judge considers critical in distinguishing A from B
</golden_key_points>
<genrm_analysis>
Analyze whether GenRM mentioned the above key points
</genrm_analysis>
<final_verdict>
Correct OR Incorrect
</final_verdict>
"""


def mock_dialogue_context(conversation_list: List[Dict[str, Any]]) -> str:
    formatted_turns: List[str] = []
    for turn in conversation_list:
        role = turn.get("role", "")
        content = turn.get("content", "")
        if role == "user":
            formatted_turns.append(f"User: {content}")
        elif role == "assistant":
            formatted_turns.append(f"Assistant: {content}")
        else:
            formatted_turns.append(f"{str(role).capitalize()}: {content}")
    return "\n".join(formatted_turns)


def format_judge_answers_custom(
    conversation_list: List[Dict[str, Any]],
    answer_a_text: str,
    answer_b_text: str,
) -> str:
    conv_context = ""
    if len(conversation_list) > 1:
        conv_context = (
            f"<|Dialogue Context|>\n{mock_dialogue_context(conversation_list[:-1])}\n\n"
            f"<|End of Dialogue Context|>\n\n"
        )

    user_last_query = ""
    if len(conversation_list) > 0:
        last_turn = conversation_list[-1]
        user_last_query = f"<|User Prompt|>\n{last_turn.get('content', '')}\n\n"
    else:
        user_last_query = "<|User Prompt|>\n\n"

    section_a = (
        f"<|The Start of Assistant A's Answer with User|>\n\n"
        f"{answer_a_text}\n\n"
        f"<|The End of Assistant A's Answer with User|>\n\n"
    )
    section_b = (
        f"<|The Start of Assistant B's Answer with User|>\n\n"
        f"{answer_b_text}\n\n"
        f"<|The End of Assistant B's Answer with User|>\n\n"
    )

    user_prompt = conv_context + user_last_query + section_a + section_b
    return user_prompt


def remove_thinking_content(response: str) -> str:
    return (response or "").split("</think>")[-1].strip()


def extract_label_from_judgment(judgment: str) -> Optional[str]:
    if not judgment:
        return None
    m = re.search(r"\\boxed\{([AB])\}", judgment)
    if m:
        return m.group(1)
    m = re.search(r"boxed\{([AB])\}", judgment)
    if m:
        return m.group(1)
    return None


def post_process_metarm(response: str) -> Tuple[bool, float, str]:
    """Parse metaRM output. Returns: (ok, score, metarm_analysis)."""
    try:
        meta_judge = response.split("<genrm_analysis>")[1].split("</genrm_analysis>")[0].strip()
        final_verdict = response.split("<final_verdict>")[1].split("</final_verdict>")[0].strip()
        if final_verdict.strip().lower() == "correct":
            return True, 1.0, meta_judge
        return True, 0.0, meta_judge
    except Exception:
        return False, 0.0, ""


def _sanitize_model_name_for_filename(model: str) -> str:
    s = (model or "").strip()
    if not s:
        return "unknown_model"
    s = s.replace("/", "__").replace("\\", "__")
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^0-9A-Za-z._\-]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "unknown_model"


def _sanitize_suffix_for_filename(suffix: str) -> str:
    s = (suffix or "").strip()
    if not s:
        return ""
    s = s.replace("/", "__").replace("\\", "__")
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^0-9A-Za-z._\-]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s


def _append_suffix_to_path(output_path: Path, suffix: str) -> Path:
    sfx = _sanitize_suffix_for_filename(suffix)
    if not sfx:
        return output_path
    return output_path.with_name(f"{output_path.stem}{sfx}{output_path.suffix}")


def _derive_output_path(input_path: Path, metarm_model: str) -> Path:
    model_part = _sanitize_model_name_for_filename(metarm_model)
    out_dir = Path(__file__).resolve().parent / "result_metarm"
    return out_dir / f"{input_path.stem}_metarm_{model_part}.jsonl"


def _build_context_and_responses(item: Dict[str, Any]) -> str:
    conversations = item["conversations"]
    answer_a_text = item["response_A"]
    answer_b_text = item["response_B"]
    return format_judge_answers_custom(conversations, answer_a_text, answer_b_text)


def _build_metarm_prompt(item: Dict[str, Any]) -> str:
    context_and_responses = _build_context_and_responses(item)
    golden_explanation = remove_thinking_content(item.get("gt_judgment", "") or "")
    genrm_explanation = remove_thinking_content(item.get("model_judgment", "") or "")
    return META_JUDGE_PROMPT_V1.format(
        context_and_responses=context_and_responses,
        golden_explanation=golden_explanation,
        genrm_explanation=genrm_explanation,
    )


def _call_metarm_with_retry(
    item: Dict[str, Any],
    *,
    metarm_model: str,
    base_url: str,
    api_key: Optional[str],
    temperature: float,
    max_tokens: int,
    outer_retries: int,
) -> Tuple[Optional[str], Optional[str]]:
    prompt = _build_metarm_prompt(item)
    msg = [{"role": "user", "content": prompt}]

    last_error: Optional[str] = None
    for attempt in range(outer_retries):
        try:
            response_text = request_internal_conv_api(
                conversation=msg,
                internal_api_model=metarm_model,
                temperature=temperature,
                max_tokens=max_tokens,
                base_url=base_url,
                api_key=api_key,
                return_raw_requst_res=False,
                concat_resoning_content=True,
            )
            if response_text and len(response_text) > 0:
                return response_text, None
            last_error = "empty_response"
        except Exception as e:
            last_error = f"api_error: {e}"

        if attempt < outer_retries - 1:
            time.sleep(random.uniform(1, 3))

    return None, last_error


def get_args() -> argparse.Namespace:
    p = argparse.ArgumentParser()
    p.add_argument(
        "--input_jsonl",
        type=str,
        required=True,
        help="Input JSONL (must include model_judgment/model_pred/gt_label/gt_judgment).",
    )
    p.add_argument("--model", type=str, default=DEFAULT_METARM_MODEL, help="metaRM judge model name.")
    p.add_argument("--base_url", type=str, default=os.environ.get("OPENAI_BASE_URL", ""))
    p.add_argument("--api_key", type=str, default=os.environ.get("OPENAI_API_KEY", ""))
    p.add_argument("--temperature", type=float, default=DEFAULT_TEMPERATURE)
    p.add_argument("--max_tokens", type=int, default=DEFAULT_MAX_TOKENS)
    p.add_argument("--num_threads", type=int, default=DEFAULT_NUM_THREADS)
    p.add_argument("--retries", type=int, default=DEFAULT_OUTER_RETRIES)
    p.add_argument("--overwrite", action="store_true", help="Allow overwriting existing output file.")
    p.add_argument("--debug", action="store_true", help="Only run the first 10 samples.")
    p.add_argument(
        "--output_jsonl",
        type=str,
        default="",
        help="Optional: explicit output path (default: pub_aug_bmk/result_metarm/).",
    )
    p.add_argument(
        "--suffix",
        type=str,
        default="",
        help="Optional: filename suffix to append before extension (e.g., _v2).",
    )
    return p.parse_args()


def main() -> None:
    args = get_args()
    input_path = Path(args.input_jsonl)
    if not input_path.exists():
        raise FileNotFoundError(f"input_jsonl not found: {input_path}")
    if not args.base_url:
        raise ValueError("base_url is required (pass --base_url or set OPENAI_BASE_URL).")

    output_path = Path(args.output_jsonl) if args.output_jsonl else _derive_output_path(input_path, args.model)
    output_path = _append_suffix_to_path(output_path, args.suffix)
    if output_path.exists() and not args.overwrite:
        raise FileExistsError(
            f"output_jsonl already exists: {output_path}\n"
            "Refusing to overwrite. Use --overwrite if you really want to overwrite."
        )

    print(f"[config] input_jsonl : {input_path}")
    print(f"[config] output_jsonl: {output_path}")
    print(
        f"[config] model={args.model} base_url={args.base_url} "
        f"temp={args.temperature} max_tokens={args.max_tokens}"
    )
    print(f"[config] num_threads={args.num_threads} retries={args.retries}")
    if args.suffix:
        print(f"[config] suffix={args.suffix}")
    if args.debug:
        print("[config] debug=True (only first 10 samples)")

    output_path.parent.mkdir(parents=True, exist_ok=True)

    items: List[Dict[str, Any]] = []
    total_bad_json = 0
    with open(input_path, "r", encoding="utf-8") as in_f:
        for line in in_f:
            line = line.strip()
            if not line:
                continue
            try:
                items.append(json.loads(line))
            except Exception:
                total_bad_json += 1
                continue

    if args.debug:
        items = items[:10]

    call_indices: List[int] = []
    pred_labels: List[Optional[str]] = [None for _ in range(len(items))]
    for i, item in enumerate(items):
        pred = item.get("model_pred")
        if pred not in ("A", "B"):
            pred = extract_label_from_judgment(item.get("model_judgment", "") or "")
        pred_labels[i] = pred
        if pred is None:
            continue
        if (item.get("gt_label") or "").strip() == pred:
            call_indices.append(i)

    metarm_raw_list: List[Optional[str]] = [None for _ in range(len(items))]
    metarm_err_list: List[Optional[str]] = [None for _ in range(len(items))]

    if call_indices:
        with ThreadPoolExecutor(max_workers=args.num_threads) as executor:
            future_to_i: Dict[Any, int] = {}
            for i in call_indices:
                fut = executor.submit(
                    _call_metarm_with_retry,
                    items[i],
                    metarm_model=args.model,
                    base_url=args.base_url,
                    api_key=args.api_key or None,
                    temperature=args.temperature,
                    max_tokens=args.max_tokens,
                    outer_retries=args.retries,
                )
                future_to_i[fut] = i

            with tqdm(total=len(call_indices), desc="metaRM judging") as pbar:
                for future in as_completed(future_to_i):
                    i = future_to_i[future]
                    try:
                        metarm_raw_list[i], metarm_err_list[i] = future.result()
                    except Exception as e:
                        metarm_raw_list[i] = None
                        metarm_err_list[i] = f"future_exception: {e}"
                    pbar.update(1)

    total_written = 0
    total_call = len(call_indices)
    total_call_fail = 0
    total_parse_fail = 0
    total_skipped = 0
    total_match = 0

    with open(output_path, "w", encoding="utf-8") as out_f:
        for i, item in enumerate(items):
            out_item = copy.deepcopy(item)

            gt_label = (out_item.get("gt_label") or "").strip()
            pred = pred_labels[i]

            metarm_raw = metarm_raw_list[i]
            metarm_err = metarm_err_list[i]

            metarm_verdict: str
            metarm_score: Optional[float]
            metarm_analysis: str = ""

            if pred is None or pred != gt_label:
                metarm_verdict = "SkippedLabelMismatch"
                metarm_score = None
                total_skipped += 1
            else:
                if metarm_raw is None:
                    metarm_verdict = "CallFail"
                    metarm_score = None
                    total_call_fail += 1
                else:
                    ok, score, analysis = post_process_metarm(metarm_raw)
                    if not ok:
                        metarm_verdict = "ParseFail"
                        metarm_score = None
                        total_parse_fail += 1
                    else:
                        metarm_score = score
                        metarm_analysis = analysis
                        metarm_verdict = "Correct" if score >= 0.5 else "Incorrect"
                        if score >= 0.5:
                            total_match += 1

            out_item["metarm_raw"] = metarm_raw
            out_item["metarm_analysis"] = metarm_analysis
            out_item["metarm_verdict"] = metarm_verdict
            out_item["metarm_score"] = metarm_score

            if not isinstance(out_item.get("meta"), dict):
                out_item["meta"] = {}
            if metarm_err:
                out_item["meta"]["metarm_error"] = metarm_err
            out_item["meta"]["metarm_model_name"] = args.model

            out_f.write(json.dumps(out_item, ensure_ascii=False) + "\n")
            total_written += 1

    print(
        "[done] "
        f"read={len(items)} written={total_written} bad_json={total_bad_json} "
        f"call={total_call} call_fail={total_call_fail} parse_fail={total_parse_fail} "
        f"skipped={total_skipped} match={total_match}"
    )
    print(f"[done] saved to: {output_path}")


if __name__ == "__main__":
    main()

